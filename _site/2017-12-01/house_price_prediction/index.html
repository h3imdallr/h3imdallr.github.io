<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>House Price Prediction (kaggle)</title>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" type="text/css">

  <!-- Font -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel="stylesheet">

  <link rel="alternate" type="application/rss+xml" title="RSS Feed for Gradient Descent" href="/feed.xml" />
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>House Price Prediction (kaggle) | Gradient Descent</title>
<meta name="generator" content="Jekyll v3.7.2" />
<meta property="og:title" content="House Price Prediction (kaggle)" />
<meta name="author" content="Josh Yongmin Jung" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Regularized Regression &amp; XGBoost" />
<meta property="og:description" content="Regularized Regression &amp; XGBoost" />
<link rel="canonical" href="http://localhost:4000/2017-12-01/house_price_prediction/" />
<meta property="og:url" content="http://localhost:4000/2017-12-01/house_price_prediction/" />
<meta property="og:site_name" content="Gradient Descent" />
<meta property="og:image" content="http://localhost:4000/figures-h3imdallr/20161123-header-anomaly.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-12-01T09:00:00+09:00" />
<script type="application/ld+json">
{"description":"Regularized Regression &amp; XGBoost","author":{"@type":"Person","name":"Josh Yongmin Jung"},"@type":"BlogPosting","url":"http://localhost:4000/2017-12-01/house_price_prediction/","image":"http://localhost:4000/figures-h3imdallr/20161123-header-anomaly.jpg","headline":"House Price Prediction (kaggle)","dateModified":"2017-12-01T09:00:00+09:00","datePublished":"2017-12-01T09:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017-12-01/house_price_prediction/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');

</script>



</head>

<body>
  <div class="content-container">
    <header>
  <div class="header-small">
    <a href="http://localhost:4000">Gradient Descent</a>
  </div>
</header>
<div class="post">
  <div class="post-title">House Price Prediction (kaggle)</div>
  <span class="post-date">
    <time>01 Dec 2017</time>
  </span>
  <div class="post-tag">
    <ul>
      
      <li>
        <a href="http://localhost:4000/tags#data_science">
          <span>data_science</span>
        </a>
      </li>
      
      
      <li>
        <a href="http://localhost:4000/tags#time_series">
          <span>time_series</span>
        </a>
      </li>
      
      
    </ul>
  </div>

  <h3 id="--objectives">- Objectives</h3>
<p>Predict the house price given vairous features of dataset.</p>

<h3 id="--subgoals">- Subgoals</h3>
<ul>
  <li>Exploratory data analysis (EDA)/ Preprocessing
. Histogram<br />
. Normality/Skewness<br />
. Missing values<br />
. Correlations among features<br />
. Outliers</li>
  <li>Feature Selection (for predictors)<br />
. correlation matrix<br />
. K-best<br />
. ANOVA test for categorical features</li>
  <li>Modeling<br />
. regularized regressoin - LASSO, ElasticNet<br />
. XGBoost Regression</li>
  <li>Others<br />
. categorical predictor by one-hot encoding</li>
</ul>

<h3 id="--references">- References</h3>
<ul>
  <li>feature selection: http://scikit-learn.org/stable/modules/feature_selection.html</li>
  <li><a href="https://www.kaggle.com/dansbecker/learning-to-use-xgboost">exmaple from kaggle-1</a></li>
  <li><a href="https://www.kaggle.com/apapiu/regularized-linear-models">exmaple from kaggle-2</a></li>
  <li><a href="https://www.kaggle.com/fiorenza2/journey-to-the-top-10">exmaple from kaggle-3</a></li>
  <li><a href="https://datascienceschool.net/view-notebook/a60e97ad90164e07ad236095ca74e657/">ANOVA</a></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># import data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./train.csv'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="1-eda">1. EDA</h2>

<p>** a. Histogram **
check out histogram for some features</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#histogram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OverallQual'</span><span class="p">],</span><span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">],</span><span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OverallCond'</span><span class="p">],</span><span class="n">kde</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11b8114e0&gt;
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_4_1.png" alt="png" /></p>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_4_2.png" alt="png" /></p>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_4_3.png" alt="png" /></p>

<p>** b. Normality Check **</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># - check with skewness and kurtosi</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">- Skewness of SalePrice histogram: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">.</span><span class="n">skew</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">- Kurtosis of SalePrice histogram: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">.</span><span class="n">kurt</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Skewness of SalePrice histogram: 1.882876

- Kurtosis of SalePrice histogram: 6.536282
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># - check with Q-Q Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">));</span> <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">],</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((array([-3.30513952, -3.04793228, -2.90489705, ...,  2.90489705,
          3.04793228,  3.30513952]),
  array([ 34900,  35311,  37900, ..., 625000, 745000, 755000])),
 (74160.164745194154, 180921.19589041095, 0.93196656415129864))
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_7_1.png" alt="png" /></p>

<p>you can simply normalized by taking a logarithm. check out Q-Q plot for normality</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># - normalize</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">));</span> <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]),</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">));</span> <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]),</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((array([-3.30513952, -3.04793228, -2.90489705, ...,  2.90489705,
          3.04793228,  3.30513952]),
  array([ 10.46024211,  10.47194981,  10.54270639, ...,  13.34550693,
          13.5211395 ,  13.53447303])),
 (0.39826223081618878, 12.024050901109383, 0.99537614756366133))
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_9_1.png" alt="png" /></p>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_9_2.png" alt="png" /></p>

<p>as the ‘OverallQual(categorical value)’ seems to be the easiest guess for the predictor for ‘SalePrice’, we can check the relation between them by boxplot. you can see the price gets higer with better overall quality.</p>

<p>** c. Correlations **</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OverallQual'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'steelblue'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'OverallQual'</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'SalePrice'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/Josh/anaconda/envs/venv_py35/lib/python3.5/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.
  warnings.warn("No labelled objects found. "





&lt;matplotlib.text.Text at 0x11c3023c8&gt;
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_12_2.png" alt="png" /></p>

<p>we can check the relations between some features to price as belows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'SaleCondition'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'LotShape'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'OverallQual'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x11ceda9b0&gt;
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_14_1.png" alt="png" /></p>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_14_2.png" alt="png" /></p>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_14_3.png" alt="png" /></p>

<p>** d. Missing Values **</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s"> - Missinv Value portion: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> - Missinv Value portion:
 PoolQC           0.995205
MiscFeature      0.963014
Alley            0.937671
Fence            0.807534
FireplaceQu      0.472603
LotFrontage      0.177397
GarageCond       0.055479
GarageType       0.055479
GarageYrBlt      0.055479
GarageFinish     0.055479
GarageQual       0.055479
BsmtExposure     0.026027
BsmtFinType2     0.026027
BsmtFinType1     0.025342
BsmtCond         0.025342
BsmtQual         0.025342
MasVnrArea       0.005479
MasVnrType       0.005479
Electrical       0.000685
Utilities        0.000000
YearRemodAdd     0.000000
MSSubClass       0.000000
Foundation       0.000000
ExterCond        0.000000
ExterQual        0.000000
Exterior2nd      0.000000
Exterior1st      0.000000
RoofMatl         0.000000
RoofStyle        0.000000
YearBuilt        0.000000
                   ...   
GarageArea       0.000000
PavedDrive       0.000000
WoodDeckSF       0.000000
OpenPorchSF      0.000000
3SsnPorch        0.000000
BsmtUnfSF        0.000000
ScreenPorch      0.000000
PoolArea         0.000000
MiscVal          0.000000
MoSold           0.000000
YrSold           0.000000
SaleType         0.000000
Functional       0.000000
TotRmsAbvGrd     0.000000
KitchenQual      0.000000
KitchenAbvGr     0.000000
BedroomAbvGr     0.000000
HalfBath         0.000000
FullBath         0.000000
BsmtHalfBath     0.000000
BsmtFullBath     0.000000
GrLivArea        0.000000
LowQualFinSF     0.000000
2ndFlrSF         0.000000
1stFlrSF         0.000000
CentralAir       0.000000
SaleCondition    0.000000
Heating          0.000000
TotalBsmtSF      0.000000
Id               0.000000
Length: 81, dtype: float64
</code></pre></div></div>

<p>Dealing with missing values: take out features with too many missing values, and fill mean values for rest of NaNs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Take out columns with too many NaNs</span>
<span class="n">col_nan</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[:</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">col_nan</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># replace with mean of each column for the rest of the features</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

</code></pre></div></div>

<h2 id="2--feature-selection">2.  Feature Selection</h2>

<p>** a. Correlation Matrix **</p>
<ul>
  <li>high correlation among predictors –&gt; room for feature reduction</li>
  <li>high correlation between predictors and dependant variable –&gt; important feature</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" (1) feature select by correlation matrix """</span>
<span class="c"># - the function automatically only applies for numerical data</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="c"># print (corr_matrix)</span>

<span class="c"># narrow down features to most correlated features with 'SalePrice'</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"- Correlation Matrix:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">corr_matrix</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">- Most relavant features by correlation matrix:</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
       <span class="n">corr_matrix</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Correlation Matrix:
 Id              -0.021917
MSSubClass      -0.084284
LotArea          0.263843
OverallQual      0.790982
OverallCond     -0.077856
YearBuilt        0.522897
YearRemodAdd     0.507101
MasVnrArea       0.475241
BsmtFinSF1       0.386420
BsmtFinSF2      -0.011378
BsmtUnfSF        0.214479
TotalBsmtSF      0.613581
1stFlrSF         0.605852
2ndFlrSF         0.319334
LowQualFinSF    -0.025606
GrLivArea        0.708624
BsmtFullBath     0.227122
BsmtHalfBath    -0.016844
FullBath         0.560664
HalfBath         0.284108
BedroomAbvGr     0.168213
KitchenAbvGr    -0.135907
TotRmsAbvGrd     0.533723
Fireplaces       0.466929
GarageYrBlt      0.470177
GarageCars       0.640409
GarageArea       0.623431
WoodDeckSF       0.324413
OpenPorchSF      0.315856
EnclosedPorch   -0.128578
3SsnPorch        0.044584
ScreenPorch      0.111447
PoolArea         0.092404
MiscVal         -0.021190
MoSold           0.046432
YrSold          -0.028923
SalePrice        1.000000
Name: SalePrice, dtype: float64

- Most relavant features by correlation matrix:
 Index(['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',
       'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',
       'YearRemodAdd'],
      dtype='object')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># narrow down features to most correlated features with 'SalePrice'</span>
<span class="n">new_corr</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">corr_matrix</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c"># check with heatmap w/ new corr matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">new_corr</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">7</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">7</span><span class="p">);</span>
<span class="c"># plt.legend(); plt.xlabel(''); plt.ylabel('')</span>
<span class="c"># plt.savefig('./figures/heatmap.png'); plt.close(</span>
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_21_0.png" alt="png" /></p>

<p>** b. K-Best **<br />
reference: http://scikit-learn.org/stable/modules/feature_selection.html</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" (2) feature select by K-Best """</span>
<span class="c"># split categorical data and numerical data</span>
<span class="n">num_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">])</span>
<span class="n">cat_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">- among total number of features (</span><span class="si">%</span><span class="s">d), "</span>
      <span class="s">"</span><span class="se">\n</span><span class="s">-numerical features: </span><span class="si">%</span><span class="s">d </span><span class="se">\n</span><span class="s">-categorical features: </span><span class="si">%</span><span class="s">d"</span>
      <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_df</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">cat_df</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span> <span class="p">)</span>

<span class="c"># K-Best</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">mutual_info_regression</span><span class="p">,</span> <span class="n">f_regression</span>
<span class="n">predictors</span> <span class="o">=</span> <span class="n">num_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># collect all features w/o target feature</span>
<span class="n">selection</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_regression</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">predictors</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
<span class="c"># selection = SelectKBest(mutual_info_regression,k=5).fit(df[predictors], df['SalePrice'])</span>

<span class="n">scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">selection</span><span class="o">.</span><span class="n">pvalues_</span><span class="p">)</span><span class="c"># scores = selection.scores_</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- among total number of features (74),
-numerical features: 37
-categorical features: 37
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)),</span> <span class="n">scores</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">.</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">+.</span><span class="mi">2</span><span class="p">),</span> <span class="n">predictors</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s">"vertical"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c"># use top 10 most relavant features</span>
<span class="n">scores_sr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">predictors</span><span class="p">)</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">scores_sr</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"- Most relavant features by selectKbest(sklearn): </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">selected_features</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Most relavant features by selectKbest(sklearn):
 Index(['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF',
       '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd'],
      dtype='object')
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_24_1.png" alt="png" /></p>

<p>** c. ANOVA test for “CATEGORICAL data” **</p>
<ul>
  <li>The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean.</li>
  <li>해당 feature내의 범주값/그룹 간의 차이가 서로 다른 성격의 그룹이라고 볼 수 있을 정도인지 확인 할 수 있음</li>
  <li>The ANOVA test has important assumptions that must be satisfied in order for the associated p-value to be valid.<br />
  . The samples are independent.<br />
  . Each sample is from a normally distributed population.<br />
  . The population standard deviations of the groups are all equal. This property is known as homoscedasticity.</li>
</ul>

<p>//</p>
<ul>
  <li>reference (KOR): https://datascienceschool.net/view-notebook/a60e97ad90164e07ad236095ca74e657/</li>
  <li>reference: https://www.kaggle.com/tamatoa/house-prices-predicting-sales-price</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">col_cat</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">cat_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="c">#print(cat)</span>
<span class="k">def</span> <span class="nf">anova_test</span><span class="p">(</span><span class="n">inDF</span><span class="p">):</span>
    <span class="n">anv</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">anv</span><span class="p">[</span><span class="s">'features'</span><span class="p">]</span> <span class="o">=</span> <span class="n">col_cat</span>
    <span class="n">pvals</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">col_cat</span><span class="p">:</span>
        <span class="n">samples</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">cls</span> <span class="ow">in</span> <span class="n">inDF</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
            <span class="n">s</span><span class="o">=</span><span class="n">inDF</span><span class="p">[</span><span class="n">inDF</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">==</span><span class="n">cls</span><span class="p">][</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">pval</span><span class="o">=</span><span class="n">stats</span><span class="o">.</span><span class="n">f_oneway</span><span class="p">(</span><span class="o">*</span><span class="n">samples</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pvals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pval</span><span class="p">)</span>
    <span class="n">anv</span><span class="p">[</span><span class="s">"pval"</span><span class="p">]</span><span class="o">=</span><span class="n">pvals</span>
    <span class="k">return</span> <span class="n">anv</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s">"pval"</span><span class="p">)</span>

<span class="n">cat_df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">SalePrice</span><span class="o">.</span><span class="n">values</span>
<span class="n">k</span><span class="o">=</span><span class="n">anova_test</span><span class="p">(</span><span class="n">cat_df</span><span class="p">)</span>
<span class="n">k</span><span class="p">[</span><span class="s">'disparity'</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">k</span><span class="p">[</span><span class="s">'pval'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="s">"features"</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">"disparity"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Users/Josh/anaconda/envs/venv_py35/lib/python3.5/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
/Users/Josh/anaconda/envs/venv_py35/lib/python3.5/site-packages/scipy/stats/stats.py:2958: RuntimeWarning: invalid value encountered in double_scalars
  ssbn += _square_of_sums(a - offset) / float(len(a))
</code></pre></div></div>

<p><img src="/figures-h3imdallr/house_price_prediction_files/house_price_prediction_26_1.png" alt="png" /></p>

<h2 id="3-modeling--4-evaluation--regularized-regression--xgboost-regression">3. Modeling &amp; 4. Evaluation / Regularized Regression &amp; XGBoost Regression</h2>

<p>** a. Preprocessing**</p>
<ul>
  <li>normalize skewed data –&gt; <strong>* note that log1p is used otherwise it will result in “infinite” in the table</strong>*</li>
  <li>deal with missing values</li>
  <li>deal with outliers</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># unskew all data</span>
<span class="k">print</span><span class="p">(</span><span class="n">num_df</span><span class="o">.</span><span class="n">skew</span><span class="p">())</span>
<span class="n">skewed_features</span> <span class="o">=</span> <span class="n">num_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">num_df</span><span class="o">.</span><span class="n">skew</span><span class="p">()</span><span class="o">&gt;</span><span class="mf">0.75</span><span class="p">]</span>

<span class="c"># *** note that np.log1p() is used, not np.log() ***</span>
<span class="c"># if np.log() is used, the dataframe is filled with -inf</span>
<span class="c"># http://rfriend.tistory.com/295</span>
<span class="n">num_df_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">num_df</span><span class="p">[</span><span class="n">skewed_features</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Id                0.000000
MSSubClass        1.407657
LotArea          12.207688
OverallQual       0.216944
OverallCond       0.693067
YearBuilt        -0.613461
YearRemodAdd     -0.503562
MasVnrArea        2.676412
BsmtFinSF1        1.685503
BsmtFinSF2        4.255261
BsmtUnfSF         0.920268
TotalBsmtSF       1.524255
1stFlrSF          1.376757
2ndFlrSF          0.813030
LowQualFinSF      9.011341
GrLivArea         1.366560
BsmtFullBath      0.596067
BsmtHalfBath      4.103403
FullBath          0.036562
HalfBath          0.675897
BedroomAbvGr      0.211790
KitchenAbvGr      4.488397
TotRmsAbvGrd      0.676341
Fireplaces        0.649565
GarageYrBlt      -0.668175
GarageCars       -0.342549
GarageArea        0.179981
WoodDeckSF        1.541376
OpenPorchSF       2.364342
EnclosedPorch     3.089872
3SsnPorch        10.304342
ScreenPorch       4.122214
PoolArea         14.828374
MiscVal          24.476794
MoSold            0.212053
YrSold            0.096269
SalePrice         1.882876
dtype: float64
</code></pre></div></div>

<p>Split predictors(X_train) &amp; dependant variable (y; SalePrice)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span>  <span class="o">=</span> <span class="n">num_df_norm</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'SalePrice'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">num_df_norm</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span>
</code></pre></div></div>

<p>** a. LASSO regressoin **</p>

<p>note that R^2 score is used as evaluation matrix. If we splited data for test set, RMSE can be used.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" (1)-a Regularized Regression : LASSO """</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span><span class="p">,</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">model_lasso</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LASSO/alpha: "</span><span class="p">,</span><span class="n">model_lasso</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LASSO/Coef: "</span><span class="p">,</span><span class="n">model_lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LASSO/R^2 Score: "</span><span class="p">,</span><span class="n">model_lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LASSO/alpha:  0.000441380629663
LASSO/Coef:  [ 0.02133863  0.06319689  0.01107165  0.01133217 -0.01055398 -0.00459845
  0.03339088 -0.14267849 -0.04117765 -0.05167887  1.02319654 -0.03696758
 -0.67642781  0.016846    0.02119312 -0.01589131  0.0071636   0.00587626
 -0.01605986 -0.00734329]
LASSO/R^2 Score:  0.746881679139
</code></pre></div></div>

<p>** b. ElasticNet **</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" (1)-b Regularized Regression : Elastic Net """</span>
<span class="n">model_elastic</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"ElasticNet/alpha: "</span><span class="p">,</span><span class="n">model_elastic</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"ElasticNet/Coef: "</span><span class="p">,</span><span class="n">model_elastic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"ElasticNet/R^2 Score: "</span><span class="p">,</span><span class="n">model_elastic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ElasticNet/alpha:  0.000882761259325
ElasticNet/Coef:  [ 0.01897788  0.06288092  0.01173252  0.01134066 -0.0105054  -0.00453945
  0.03448621 -0.07296666 -0.03380719 -0.04787024  0.93591079 -0.03637207
 -0.62223445  0.01718832  0.02206114 -0.01598137  0.00742926  0.00610746
 -0.01617787 -0.00747223]
ElasticNet/R^2 Score:  0.745316818387
</code></pre></div></div>

<p>** c. XGBoost**<br />
note that thise time test set (with SalePrice known) is prepared and the RMSE is also used for evaluation matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'./train.csv'</span><span class="p">)</span>

<span class="c"># handling with NaNs</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">])</span>
<span class="n">col_nan</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[:</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">col_nan</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">])</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'SalePrice'</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'SalePrice'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="n">model_XGboost</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="s">"""Evaluation"""</span>
<span class="c"># a. Mean Squared Error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>
<span class="n">y_predict</span> <span class="o">=</span> <span class="n">model_XGboost</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c"># print ("-MAE: ", mean_absolute_error(y_test, y_predict))</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"- RMSE: "</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predict</span><span class="p">))</span> <span class="p">)</span>

<span class="c"># (2) r^2</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="k">print</span><span class="p">(</span><span class="s">"- r^2:"</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predict</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- RMSE:  28337.5912785
- r^2: 0.890929189221


/Users/Josh/anaconda/envs/venv_py35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" tuning more parameters"""</span>
<span class="n">model_XGboost_tuned</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
         <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)],</span><span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">y_predict_tuned</span> <span class="o">=</span> <span class="n">model_XGboost_tuned</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">"- RMSE/tuned model: "</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predict_tuned</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"- r^2/tuned model:"</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_predict_tuned</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- RMSE/tuned model:  28711.3952045
- r^2/tuned model: 0.888032682978
</code></pre></div></div>

<h2 id="further-notes">Further Notes</h2>
<p>To improve the model, among lots of methods, you can take following steps:</p>
<ul>
  <li>use more features including categorial data by encoding the feature (e.g. one-hot encoding)
<a href="https://datascienceschool.net/view-notebook/7dda1bc9ad1c435fb309ea88f672eff9/">referece(KOR)</a></li>
  <li>another exmaple finding a categorical feature to improve the model <a href="https://www.kaggle.io/svf/1353045/81ffe596e9a69d92e19c83a2209e270f/__results__.html#4.-Categoric-to-Numeric">example</a></li>
</ul>

<p>** // End of the document **</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>


  <!-- Disqus -->
  

</div>


    <!-- Documents about icons are here: http://fontawesome.io/icons/ -->
<div class="footer">
  <hr />
  <div class="footer-link">
    
	
	
	
	

    

    
    <a href="https://github.com/h3imdallr"><i class="fa fa-github" aria-hidden="true"></i></a>
    
	
	
	
	

    
    <a href="https://www.linkedin.com/in/joshjung"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
    
	
	
	
	
	
	
	
	

    

    

    

  </div>
  © 2017 h3imdallr. All rights reserved.
</div>

  </div>
</body>
</html>
